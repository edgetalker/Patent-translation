"""
è¯­æ–™åº“æ£€ç´¢æ¨¡å—
è´Ÿè´£å¥å­çº§åˆ«çš„ç›¸ä¼¼è¯­æ–™æ£€ç´¢å’Œç»“æœç»„ç»‡
"""
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
import re
from corpus.manager import CorpusManager


# ==================== æ•°æ®ç»“æ„ ====================

@dataclass
class SentenceMatch:
    """å•å¥åŒ¹é…ç»“æœ"""
    index: int              # å¥å­åœ¨chunkä¸­çš„ä½ç½®
    source: str             # æºå¥å­
    matched: bool           # æ˜¯å¦å‘½ä¸­è¯­æ–™åº“
    translation: str        # è¯‘æ–‡ï¼ˆå‘½ä¸­æ—¶æœ‰å€¼ï¼Œæœªå‘½ä¸­ä¸ºç©ºï¼‰
    similarity: float       # ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0.0-1.0ï¼‰
    corpus_source: str      # å‘½ä¸­çš„è¯­æ–™åº“åŸæ–‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰


@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    sentences: List[SentenceMatch]
    hit_count: int
    miss_count: int
    hit_rate: float
    
    def get_unmatched_sentences(self) -> List[Tuple[int, str]]:
        """
        è·å–æœªå‘½ä¸­çš„å¥å­
        
        Returns:
            List[(index, sentence)]
        """
        return [
            (sent.index, sent.source)
            for sent in self.sentences
            if not sent.matched
        ]
    
    def get_matched_translations(self) -> Dict[int, str]:
        """
        è·å–å·²å‘½ä¸­å¥å­çš„ç¿»è¯‘
        
        Returns:
            {index: translation}
        """
        return {
            sent.index: sent.translation
            for sent in self.sentences
            if sent.matched
        }


# ==================== è¯­è¨€é…ç½® ====================

class LanguageConfig:
    """å¤šè¯­è¨€åˆ†å¥é…ç½®"""
    
    # ä¸åŒè¯­è¨€çš„å¥å­è¾¹ç•Œç¬¦
    SENTENCE_DELIMITERS = {
        'zh': ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›'],      # ä¸­æ–‡
        'en': ['. ', '! ', '? ', '; '],     # è‹±æ–‡ï¼ˆæ³¨æ„ç©ºæ ¼ï¼‰
        'ja': ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›'],      # æ—¥æ–‡
        'de': ['. ', '! ', '? ', '; '],     # å¾·è¯­
        'fr': ['. ', '! ', '? ', '; '],     # æ³•è¯­
    }
    
    # å›ºå®šå¥å¼æ¨¡å¼ï¼ˆç”¨äºæé«˜é˜ˆå€¼ï¼‰
    FIXED_PATTERNS = {
        'zh': [
            r'^æœ¬å‘æ˜æ¶‰åŠ',
            r'^æ‰€è¿°\w+åŒ…æ‹¬',
            r'^æ ¹æ®æœ¬å‘æ˜',
            r'^å¦‚å›¾\d+',
            r'^ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”',
            r'^å…·ä½“å®æ–½æ–¹å¼',
            r'^åœ¨.*å®æ–½ä¾‹ä¸­',
        ],
        'en': [
            r'^The present invention relates to',
            r'^The \w+ comprises',
            r'^According to the present invention',
            r'^As shown in (FIG\.|Figure)',
            r'^Compared with the prior art',
            r'^In an embodiment',
        ],
        'ja': [
            r'^æœ¬ç™ºæ˜ã¯',
            r'^å‰è¨˜\w+ã¯',
            r'^å›³\d+ã«ç¤ºã™',
        ]
    }
    
    @classmethod
    def get_delimiters(cls, lang: str) -> List[str]:
        """è·å–è¯­è¨€çš„åˆ†å¥ç¬¦"""
        return cls.SENTENCE_DELIMITERS.get(lang, cls.SENTENCE_DELIMITERS['en'])
    
    @classmethod
    def get_fixed_patterns(cls, lang: str) -> List[str]:
        """è·å–å›ºå®šå¥å¼æ¨¡å¼"""
        return cls.FIXED_PATTERNS.get(lang, [])


# ==================== åˆ†å¥å™¨ ====================

class SentenceSplitter:
    """å¤šè¯­è¨€åˆ†å¥å™¨"""
    
    def __init__(self, lang: str = 'zh'):
        """
        Args:
            lang: è¯­è¨€ä»£ç  ('zh', 'en', 'ja' ç­‰)
        """
        self.lang = lang
        self.delimiters = LanguageConfig.get_delimiters(lang)
    
    def split(self, text: str, min_length: int = 5, max_length: int = 500) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å¥
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            min_length: æœ€å°å¥å­é•¿åº¦ï¼ˆè¿‡æ»¤å¤ªçŸ­çš„å¥å­ï¼‰
            max_length: æœ€å¤§å¥å­é•¿åº¦ï¼ˆè¶…é•¿å¥å­ä¼šè¢«å¼ºåˆ¶åˆ†å‰²ï¼‰
        
        Returns:
            å¥å­åˆ—è¡¨
        """
        # æŒ‰åˆ†å¥ç¬¦åˆ†å‰²
        sentences = self._split_by_delimiters(text)
        
        # è¿‡æ»¤å’Œæ¸…ç†
        sentences = self._clean_sentences(sentences, min_length, max_length)
        
        return sentences
    
    def _split_by_delimiters(self, text: str) -> List[str]:
        """æŒ‰åˆ†å¥ç¬¦åˆ†å‰²"""
        sentences = [text]
        
        for delimiter in self.delimiters:
            new_sentences = []
            for sent in sentences:
                # åˆ†å‰²å¹¶ä¿ç•™åˆ†éš”ç¬¦
                parts = sent.split(delimiter)
                for i, part in enumerate(parts):
                    if i < len(parts) - 1:
                        # éæœ€åä¸€éƒ¨åˆ†ï¼ŒåŠ å›åˆ†éš”ç¬¦
                        new_sentences.append(part + delimiter.strip())
                    else:
                        # æœ€åä¸€éƒ¨åˆ†
                        if part.strip():
                            new_sentences.append(part)
            sentences = new_sentences
        
        return sentences
    
    def _clean_sentences(
        self, 
        sentences: List[str], 
        min_length: int, 
        max_length: int
    ) -> List[str]:
        """æ¸…ç†å¥å­ï¼šå»ç©ºæ ¼ã€è¿‡æ»¤å¤ªçŸ­ã€æ‹†åˆ†è¶…é•¿"""
        cleaned = []
        
        for sent in sentences:
            # å»é™¤é¦–å°¾ç©ºæ ¼
            sent = sent.strip()
            
            # è¿‡æ»¤ç©ºå¥å­
            if not sent:
                continue
            
            # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
            if len(sent) < min_length:
                continue
            
            # å¤„ç†è¶…é•¿å¥å­ï¼ˆå¼ºåˆ¶åˆ†å‰²ï¼‰
            if len(sent) > max_length:
                # å°è¯•åœ¨é€—å·å¤„åˆ†å‰²
                if 'ï¼Œ' in sent or ',' in sent:
                    sub_sents = re.split(r'[ï¼Œ,]', sent)
                    cleaned.extend([s.strip() for s in sub_sents if len(s.strip()) >= min_length])
                else:
                    # æ— æ³•åˆ†å‰²ï¼Œä¿ç•™åŸå¥
                    cleaned.append(sent)
            else:
                cleaned.append(sent)
        
        return cleaned


# ==================== è¯­æ–™åº“æ£€ç´¢å™¨ ====================

class CorpusRetriever:
    """è¯­æ–™åº“æ£€ç´¢å™¨ï¼ˆå¥å­çº§ï¼‰"""
    
    def __init__(
        self, 
        corpus_manager: Optional[CorpusManager] = None,
        src_lang: str = 'zh',
        tgt_lang: str = 'en'
    ):
        """
        Args:
            corpus_manager: è¯­æ–™åº“ç®¡ç†å™¨
            src_lang: æºè¯­è¨€ä»£ç 
            tgt_lang: ç›®æ ‡è¯­è¨€ä»£ç 
        """
        self.corpus_manager = corpus_manager
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang
        self.splitter = SentenceSplitter(lang=src_lang)
        self.fixed_patterns = LanguageConfig.get_fixed_patterns(src_lang)
    
    async def retrieve_for_chunk(
        self,
        chunk: str,
        corpus_id: str,
        threshold: float = 0.85,
        min_sentence_length: int = 5,
        max_sentence_length: int = 500
    ) -> RetrievalResult:
        """
        å¯¹å•ä¸ªchunkè¿›è¡Œå¥å­çº§æ£€ç´¢
        
        Args:
            chunk: æ–‡æœ¬å—
            corpus_id: è¯­æ–™åº“ID
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            min_sentence_length: æœ€å°å¥å­é•¿åº¦
            max_sentence_length: æœ€å¤§å¥å­é•¿åº¦
        
        Returns:
            RetrievalResult
        """
        # 1. åˆ†å¥
        sentences = self.splitter.split(
            chunk, 
            min_length=min_sentence_length,
            max_length=max_sentence_length
        )
        
        # è¿‡æ»¤ç©ºå¥å­å¹¶æ·»åŠ ç´¢å¼•
        sentences = [
            (idx, sent) 
            for idx, sent in enumerate(sentences) 
            if sent and sent.strip()
        ]
        
        if not sentences:
            return RetrievalResult(
                sentences=[],
                hit_count=0,
                miss_count=0,
                hit_rate=0.0
            )
        
        # 2. æ‰¹é‡æ£€ç´¢
        if self.corpus_manager:
            sentence_matches = await self._batch_retrieve(
                sentences=sentences,
                corpus_id=corpus_id,
                threshold=threshold
            )
        else:
            # æ— è¯­æ–™åº“ç®¡ç†å™¨ï¼Œå…¨éƒ¨æ ‡è®°ä¸ºæœªå‘½ä¸­
            sentence_matches = [
                SentenceMatch(
                    index=idx,
                    source=sent,
                    matched=False,
                    translation="",
                    similarity=0.0,
                    corpus_source=""
                )
                for idx, sent in sentences
            ]
        
        # 3. ç»Ÿè®¡
        hit_count = sum(1 for m in sentence_matches if m.matched)
        miss_count = len(sentence_matches) - hit_count
        hit_rate = hit_count / len(sentence_matches) if sentence_matches else 0.0
        
        return RetrievalResult(
            sentences=sentence_matches,
            hit_count=hit_count,
            miss_count=miss_count,
            hit_rate=hit_rate
        )
    
    async def _batch_retrieve(
        self,
        sentences: List[Tuple[int, str]],
        corpus_id: str,
        threshold: float
    ) -> List[SentenceMatch]:
        """
        æ‰¹é‡æ£€ç´¢å¥å­ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ‰¹é‡embeddingï¼‰
        
        æ€§èƒ½æå‡ï¼š100ä¸ªå¥å­ä»100æ¬¡APIè°ƒç”¨ â†’ 1æ¬¡APIè°ƒç”¨
        """
        # æå–æ‰€æœ‰å¥å­æ–‡æœ¬
        sentence_texts = [sent for idx, sent in sentences]
        
        # ğŸ”‘ æ‰¹é‡æ£€ç´¢ï¼ˆä¸€æ¬¡embedding APIè°ƒç”¨ï¼‰
        try:
            batch_results = await self.corpus_manager.batch_search_similar(
                query_texts=sentence_texts,
                corpus_id=corpus_id,
                limit=1,
                score_threshold=threshold
            )
        except Exception as e:
            print(f"âŒ æ‰¹é‡æ£€ç´¢å¤±è´¥: {str(e)}")
            # å…¨éƒ¨æ ‡è®°ä¸ºæœªå‘½ä¸­
            return [
                SentenceMatch(
                    index=idx,
                    source=sent,
                    matched=False,
                    translation="",
                    similarity=0.0,
                    corpus_source=""
                )
                for idx, sent in sentences
            ]
        
        # ç»„ç»‡ç»“æœ
        sentence_matches = []
        for (idx, sent), results in zip(sentences, batch_results):
            if results and len(results) > 0:
                # å‘½ä¸­
                best_match = results[0]
                sentence_matches.append(
                    SentenceMatch(
                        index=idx,
                        source=sent,
                        matched=True,
                        translation=best_match["target"],
                        similarity=best_match["score"],
                        corpus_source=best_match["source"]
                    )
                )
            else:
                # æœªå‘½ä¸­
                sentence_matches.append(
                    SentenceMatch(
                        index=idx,
                        source=sent,
                        matched=False,
                        translation="",
                        similarity=0.0,
                        corpus_source=""
                    )
                )
        
        return sentence_matches
    
    def _get_adaptive_threshold(self, sentence: str, base_threshold: float) -> float:
        """
        è‡ªé€‚åº”é˜ˆå€¼ï¼šå›ºå®šå¥å¼ä½¿ç”¨æ›´é«˜é˜ˆå€¼
        
        Args:
            sentence: æºå¥å­
            base_threshold: åŸºç¡€é˜ˆå€¼
        
        Returns:
            è°ƒæ•´åçš„é˜ˆå€¼
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºå›ºå®šå¥å¼
        for pattern in self.fixed_patterns:
            if re.match(pattern, sentence):
                # å›ºå®šå¥å¼ï¼Œæé«˜é˜ˆå€¼åˆ° 0.90+
                return max(base_threshold, 0.90)
        
        # æ™®é€šå¥å­ï¼Œä½¿ç”¨åŸºç¡€é˜ˆå€¼
        return base_threshold
    
    def merge_translation(
        self,
        retrieval_result: RetrievalResult,
        llm_translations: Dict[int, str]
    ) -> str:
        """
        åˆå¹¶æ£€ç´¢ç»“æœå’ŒLLMç¿»è¯‘
        
        Args:
            retrieval_result: æ£€ç´¢ç»“æœ
            llm_translations: {å¥å­ç´¢å¼•: LLMç¿»è¯‘}
        
        Returns:
            å®Œæ•´ç¿»è¯‘æ–‡æœ¬
        """
        merged_sentences = []
        
        for sent_match in retrieval_result.sentences:
            if sent_match.matched:
                # ä½¿ç”¨è¯­æ–™åº“ç¿»è¯‘
                merged_sentences.append(sent_match.translation)
            else:
                # ä½¿ç”¨LLMç¿»è¯‘
                if sent_match.index in llm_translations:
                    merged_sentences.append(llm_translations[sent_match.index])
                else:
                    # æœªæä¾›ç¿»è¯‘ï¼Œä¿ç•™åŸæ–‡ï¼ˆé”™è¯¯æƒ…å†µï¼‰
                    print(f"âš ï¸  è­¦å‘Š: å¥å­{sent_match.index}æ—¢æœªå‘½ä¸­ä¹Ÿæœªç¿»è¯‘")
                    merged_sentences.append(sent_match.source)
        
        # æ‹¼æ¥å¥å­
        # æ ¹æ®ç›®æ ‡è¯­è¨€å†³å®šåˆ†éš”ç¬¦
        if self.tgt_lang == 'zh':
            # ä¸­æ–‡ä¸éœ€è¦é¢å¤–ç©ºæ ¼
            return "".join(merged_sentences)
        else:
            # è‹±æ–‡ç­‰è¯­è¨€ï¼Œå¥å­é—´åŠ ç©ºæ ¼
            return " ".join(merged_sentences)
    
    def get_statistics(self, retrieval_result: RetrievalResult) -> Dict:
        """
        è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            {
                "total_sentences": int,
                "hit_count": int,
                "miss_count": int,
                "hit_rate": str,
                "avg_similarity": float
            }
        """
        matched_scores = [
            sent.similarity 
            for sent in retrieval_result.sentences 
            if sent.matched
        ]
        
        avg_similarity = (
            sum(matched_scores) / len(matched_scores) 
            if matched_scores else 0.0
        )
        
        return {
            "total_sentences": len(retrieval_result.sentences),
            "hit_count": retrieval_result.hit_count,
            "miss_count": retrieval_result.miss_count,
            "hit_rate": f"{retrieval_result.hit_rate * 100:.1f}%",
            "avg_similarity": round(avg_similarity, 4)
        }