"""
ç¿»è¯‘æ ¸å¿ƒæ¨¡å—
å¤„ç†é•¿æ–‡æ¡£ç¿»è¯‘å’Œæœ¯è¯­ä¸€è‡´æ€§éªŒè¯
"""
import time
import concurrent.futures
from typing import Dict, List, Tuple
from openai import OpenAI

from config import config
from utils import split_text_by_paragraph
from terminology_extraction import TerminologyExtractor


class DocumentTranslator:
    """æ–‡æ¡£ç¿»è¯‘å™¨"""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=config.LLM_API_KEY,
            base_url=config.LLM_BASE_URL
        )
        self.model_name = config.LLM_MODEL_NAME
        self.term_extractor = TerminologyExtractor()
    
    def translate_chunk(
        self,
        chunk_text: str,
        chunk_id: int,
        total_chunks: int,
        src_lang: str,
        tgt_lang: str,
        domain: str,
        term_dict: Dict[str, str] = None,
        context: str = None
    ) -> str:
        """
        ç¿»è¯‘å•ä¸ªæ–‡æœ¬å—
        
        Args:
            chunk_text: å¾…ç¿»è¯‘æ–‡æœ¬
            chunk_id: å½“å‰å—ID
            total_chunks: æ€»å—æ•°
            src_lang: æºè¯­è¨€ä»£ç 
            tgt_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            domain: é¢†åŸŸä¿¡æ¯
            term_dict: æœ¯è¯­å¯¹ç…§å­—å…¸
            context: å‰æ–‡ä¸Šä¸‹æ–‡
            
        Returns:
            ç¿»è¯‘ç»“æœ
        """
        chunk_start = time.time()
        
        src_lang_name = config.get_language_name(src_lang)
        tgt_lang_name = config.get_language_name(tgt_lang)
        
        prompt_parts = [
            f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{domain}é¢†åŸŸ{src_lang_name}-{tgt_lang_name}ç¿»è¯‘ä¸“å®¶ã€‚",
            f"\nå½“å‰æ­£åœ¨ç¿»è¯‘ç¬¬ {chunk_id + 1} æ®µï¼Œå…± {total_chunks} æ®µã€‚"
        ]
        
        #  ç®€åŒ–åŒ¹é… + æç¤ºè¯é©±åŠ¨
        if term_dict:
            # å¿«é€Ÿç²¾ç¡®åŒ¹é…ï¼ˆä»…ç”¨äºç»Ÿè®¡ï¼‰
            chunk_lower = chunk_text.lower()
            quick_matches = sum(1 for term in term_dict.keys() if term.lower() in chunk_lower)
            
            # æä¾›å®Œæ•´æœ¯è¯­è¡¨ç»™LLM
            terms_list = "\n".join([f"  - {src} â†’ {tgt}" for src, tgt in term_dict.items()])
            
            prompt_parts.append(
                f"\nã€æœ¯è¯­è¡¨ã€‘ä»¥ä¸‹æ˜¯{domain}é¢†åŸŸçš„ä¸“ä¸šæœ¯è¯­å¯¹ç…§è¡¨ï¼ˆå…±{len(term_dict)}ä¸ªï¼‰ï¼š\n{terms_list}\n"
                f"\nã€é‡è¦ç¿»è¯‘è¦æ±‚ã€‘"
                f"\n1. å½“å¾…ç¿»è¯‘æ–‡æœ¬ä¸­å‡ºç°æœ¯è¯­è¡¨ä¸­çš„æœ¯è¯­ï¼ˆåŒ…æ‹¬å…¶å˜å½¢ã€å¤æ•°ã€æ—¶æ€ã€è¯ç»„ç­‰ä»»ä½•å½¢å¼ï¼‰æ—¶ï¼Œå¿…é¡»ä¸¥æ ¼ä½¿ç”¨æŒ‡å®šçš„{tgt_lang_name}ç¿»è¯‘"
                f"\n2. æ³¨æ„è¯†åˆ«æœ¯è¯­çš„å„ç§å˜ä½“å½¢å¼ï¼š"
                f"\n   - è‹±æ–‡ï¼šå•å¤æ•°å˜åŒ–ã€åŠ¨è¯æ—¶æ€ã€æ´¾ç”Ÿè¯ç­‰ï¼ˆå¦‚ optimize/optimizes/optimization éƒ½å¯¹åº”åŒä¸€æœ¯è¯­ï¼‰"
                f"\n   - ä¸­æ–‡ï¼šè¯ç»„åŒ…å«å…³ç³»ï¼ˆå¦‚ 'æœºå™¨å­¦ä¹ ç®—æ³•' åŒ…å« 'æœºå™¨å­¦ä¹ ' æœ¯è¯­ï¼‰"
                f"\n   - å…¶ä»–è¯­è¨€ï¼šæ ¹æ®è¯¥è¯­è¨€çš„è¯­æ³•ç‰¹ç‚¹çµæ´»åŒ¹é…"
                f"\n3. å³ä½¿æœ¯è¯­åœ¨åŸæ–‡ä¸­åªä»¥éƒ¨åˆ†å½¢å¼å‡ºç°ï¼Œä¹Ÿè¦ä¿æŒè¯‘æ–‡çš„æœ¯è¯­ä¸€è‡´æ€§"
                f"\n4. å¯¹äºå¤šè¯æœ¯è¯­ï¼Œç¡®ä¿æ•´ä½“ç¿»è¯‘çš„å‡†ç¡®æ€§"
            )
            
            # ğŸ” ç®€æ´è¯Šæ–­æ—¥å¿—
            print(f" Chunk {chunk_id+1}: æœ¯è¯­è¡¨{len(term_dict)}ä¸ª, ç²¾ç¡®åŒ¹é…{quick_matches}ä¸ª â†’ LLMå°†çµæ´»åŒ¹é…å…¨éƒ¨")
        
        # æ·»åŠ ä¸Šä¸‹æ–‡
        if context:
            prompt_parts.append(f"\nã€å‰æ–‡å‚è€ƒã€‘\n{context[:200]}...\n")
        
        # æ·»åŠ ç¿»è¯‘è¦æ±‚
        prompt_parts.append(
            f"\nè¯·å°†ä»¥ä¸‹{src_lang_name}å†…å®¹ç¿»è¯‘æˆ{tgt_lang_name}ï¼š"
            "\n1. ä¸¥æ ¼éµå®ˆä¸Šè¿°æœ¯è¯­è¡¨çš„ç¿»è¯‘è§„èŒƒ"
            "\n2. ä¿æŒæ–‡æ¡£çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§"
            "\n3. ä¿æŒåŸæ–‡çš„æ®µè½ç»“æ„"
            "\n4. **ç¦æ­¢ä½¿ç”¨ä»»ä½•æ ¼å¼æ ‡è®°**ï¼šä¸è¦è¾“å‡ºmarkdownæ ¼å¼ï¼ˆå¦‚ ** __ * _ # ç­‰ï¼‰ã€HTMLæ ‡ç­¾æˆ–å…¶ä»–ä»»ä½•æ ¼å¼ç¬¦å·"
            "\n5. è¾“å‡ºçº¯æ–‡æœ¬è¯‘æ–‡ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€è¯´æ˜æˆ–æ ¼å¼ä¿®é¥°"
            f"\n\nã€å¾…ç¿»è¯‘å†…å®¹ã€‘\n{chunk_text}"
        )
        
        prompt = "".join(prompt_parts)
        prompt_time = time.time() - chunk_start
        
        for attempt in range(config.MAX_RETRIES):
            try:
                api_start = time.time()
                
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{domain}é¢†åŸŸç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿{src_lang_name}åˆ°{tgt_lang_name}çš„ç¿»è¯‘ã€‚ä½ å¯¹æœ¯è¯­çš„å„ç§å˜å½¢å½¢å¼æœ‰æ·±åˆ»ç†è§£ï¼Œèƒ½å¤Ÿçµæ´»åŒ¹é…å¹¶ä¿æŒç¿»è¯‘ä¸€è‡´æ€§ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=config.TRANSLATION_TEMPERATURE,
                    max_tokens=config.MAX_TOKENS
                )
                
                api_time = time.time() - api_start
                translation = response.choices[0].message.content.strip()
                total_time = time.time() - chunk_start
                
                print(f"   â±ï¸  Chunk {chunk_id+1} è€—æ—¶: Promptæ„å»º{prompt_time:.2f}s + APIè°ƒç”¨{api_time:.2f}s = {total_time:.2f}s")
                print(f"      è¾“å…¥{len(chunk_text)}å­— â†’ è¾“å‡º{len(translation)}å­— ({len(translation)/len(chunk_text):.2f}x)")
                
                return translation
                
            except Exception as e:
                print(f"  âš ï¸  ç¿»è¯‘chunk {chunk_id + 1} å¤±è´¥ (å°è¯• {attempt + 1}/{config.MAX_RETRIES}): {str(e)}")
                if attempt < config.MAX_RETRIES - 1:
                    time.sleep(config.RETRY_DELAY)
                else:
                    print(f"  âŒ ç¿»è¯‘chunk {chunk_id + 1} æœ€ç»ˆå¤±è´¥ï¼Œè¿”å›åŸæ–‡")
                    return f"[TRANSLATION FAILED: {chunk_text}]"
    
    def validate_terminology_consistency(
        self,
        translation: str,
        term_dict: Dict[str, str],
        src_text: str,
        tgt_lang: str
    ) -> Tuple[bool, List[str]]:
        """éªŒè¯ç¿»è¯‘ç»“æœä¸­çš„æœ¯è¯­ä¸€è‡´æ€§"""
        inconsistencies = []
        
        for src_term, tgt_term in term_dict.items():
            if src_term not in src_text:
                continue
            
            tgt_variants = [
                tgt_term,
                tgt_term.lower(),
                tgt_term.capitalize(),
            ]
            
            if tgt_lang == 'en':
                if tgt_term.endswith('y'):
                    tgt_variants.append(tgt_term[:-1] + 'ies')
                else:
                    tgt_variants.append(tgt_term + 's')
            
            found = any(variant in translation for variant in tgt_variants)
            
            if not found:
                words = tgt_term.lower().split()
                if len(words) > 1:
                    all_words_present = all(word in translation.lower() for word in words)
                    if not all_words_present:
                        inconsistencies.append(f"{src_term} -> {tgt_term}")
                else:
                    inconsistencies.append(f"{src_term} -> {tgt_term}")
        
        is_consistent = len(inconsistencies) == 0
        return is_consistent, inconsistencies
    
    def translate_document(
        self,
        src_text: str,
        src_lang: str,
        tgt_lang: str,
        domain: str = "æŠ€æœ¯",
        use_context: bool = True,
        glossary: Dict[str, str] = None,
        parallel: bool = True,
        max_workers: int = 3
    ) -> Dict:
        """
        ç¿»è¯‘é•¿æ–‡æ¡£ï¼ˆå¸¦æœ¯è¯­ä¸€è‡´æ€§å¤„ç†ï¼‰
    
        Args:
            src_text: æºæ–‡æœ¬
            src_lang: æºè¯­è¨€ä»£ç 
            tgt_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            domain: é¢†åŸŸä¿¡æ¯
            use_context: æ˜¯å¦ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆå¹¶è¡Œæ¨¡å¼ä¸‹è‡ªåŠ¨ç¦ç”¨ï¼‰
            glossary: æœ¯è¯­å¯¹ç…§å­—å…¸
            parallel: æ˜¯å¦å¯ç”¨å¹¶è¡Œç¿»è¯‘
            max_workers: å¹¶è¡Œç¿»è¯‘çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        
        Returns:
            result: åŒ…å«translation, term_dict, chunks_info, statistics
        """
        src_lang_name = config.get_language_name(src_lang)
        tgt_lang_name = config.get_language_name(tgt_lang)
    
        print(f"\n{'='*60}")
        print(f"å¼€å§‹ç¿»è¯‘é•¿æ–‡æ¡£ï¼ˆ{src_lang_name} â†’ {tgt_lang_name}ï¼ŒåŸŸï¼š{domain}ï¼‰")
        print(f"{'='*60}\n")
    
        start_time = time.time()
    
        # Step 1 & 2: æœ¯è¯­å¤„ç†
        if glossary:
            print("ğŸ“Œ æ­¥éª¤1-2: ä½¿ç”¨ä¼ å…¥çš„æœ¯è¯­è¡¨...")
            term_dict = glossary
            terms = list(glossary.keys())
            print(f"\n   âœ… ä½¿ç”¨ {len(term_dict)} ä¸ªé¢„å®šä¹‰æœ¯è¯­")
        else:
            print("ğŸ“Œ æ­¥éª¤1: ä½¿ç”¨æ»‘åŠ¨çª—å£æŠ½å–å…³é”®æœ¯è¯­...")
            terms = self.term_extractor.sliding_window_extract(src_text, src_lang, domain)
            print(f"\n   âœ… æœ€ç»ˆæå– {len(terms)} ä¸ªå…³é”®æœ¯è¯­")
            
            print(f"\nğŸ“Œ æ­¥éª¤2: å°†æœ¯è¯­ç¿»è¯‘æˆ{tgt_lang_name}...")
            term_dict = self.term_extractor.translate_terminology(terms, src_lang, tgt_lang, domain)
            print(f"   æˆåŠŸç¿»è¯‘ {len(term_dict)} ä¸ªæœ¯è¯­")
    
        # Step 3: æ–‡æ¡£åˆ†å—
        print("\nğŸ“Œ æ­¥éª¤3: æ–‡æ¡£åˆ†å—...")
        chunks = split_text_by_paragraph(src_text, config.MAX_CHUNK_LENGTH)
        print(f"   æ–‡æ¡£å·²åˆ†ä¸º {len(chunks)} ä¸ªå—")
    
        # Step 4: ç¿»è¯‘
        if parallel and len(chunks) > 2:
            print(f"\nğŸ“Œ æ­¥éª¤4: å¹¶è¡Œç¿»è¯‘ï¼ˆ{max_workers}çº¿ç¨‹å¹¶å‘ï¼ŒLLMæ™ºèƒ½æœ¯è¯­åŒ¹é…ï¼‰...")
            print(f"   âš¡ é¢„è®¡è€—æ—¶çº¦ä¸ºé¡ºåºç¿»è¯‘çš„ {1/min(max_workers, len(chunks)):.0%}\n")
            
            translations = [None] * len(chunks)
            
            def translate_task(idx):
                chunk = chunks[idx]
                return self.translate_chunk(
                    chunk_text=chunk["text"],
                    chunk_id=idx,
                    total_chunks=len(chunks),
                    src_lang=src_lang,
                    tgt_lang=tgt_lang,
                    domain=domain,
                    term_dict=term_dict,
                    context=None
                )
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_idx = {
                    executor.submit(translate_task, i): i
                    for i in range(len(chunks))
                }
                
                completed = 0
                for future in concurrent.futures.as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    try:
                        translations[idx] = future.result()
                        completed += 1
                        print(f"\n   âœ“ å®Œæˆ {completed}/{len(chunks)} ä¸ªchunks")
                    except Exception as e:
                        print(f"   âŒ Chunk {idx+1} å¤±è´¥: {e}")
                        translations[idx] = f"[TRANSLATION FAILED]"
                        completed += 1
        else:
            print(f"\nğŸ“Œ æ­¥éª¤4: é¡ºåºç¿»è¯‘ï¼ˆLLMæ™ºèƒ½æœ¯è¯­åŒ¹é…ï¼‰...")
            translations = []
            context = None
            
            for i, chunk in enumerate(chunks):
                print(f"\n   ç¿»è¯‘ Chunk {i+1}/{len(chunks)}...")
                
                if use_context and i > 0:
                    prev_translation = translations[-1]
                    context = prev_translation[-config.OVERLAP_LENGTH:] if len(prev_translation) > config.OVERLAP_LENGTH else prev_translation
                else:
                    context = None
                
                translation = self.translate_chunk(
                    chunk_text=chunk["text"],
                    chunk_id=i,
                    total_chunks=len(chunks),
                    src_lang=src_lang,
                    tgt_lang=tgt_lang,
                    domain=domain,
                    term_dict=term_dict,
                    context=context
                )
                
                translations.append(translation)
                print(f"   âœ“ å®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(translation)} å­—ç¬¦")
    
        # Step 5 & 6: ç»„è£…å’ŒéªŒè¯
        print("\nğŸ“Œ æ­¥éª¤5: ç»„è£…ç¿»è¯‘ç»“æœ...")
        full_translation = "\n\n".join(translations)
    
        print("\nğŸ“Œ æ­¥éª¤6: éªŒè¯æœ¯è¯­ä¸€è‡´æ€§...")
        is_consistent, inconsistencies = self.validate_terminology_consistency(
            full_translation, term_dict, src_text, tgt_lang
        )
    
        if is_consistent:
            print("   âœ… æ‰€æœ‰æœ¯è¯­ç¿»è¯‘ä¸€è‡´")
        else:
            print(f"   âš ï¸  å‘ç° {len(inconsistencies)} ä¸ªæœ¯è¯­å¯èƒ½æœªæ­£ç¡®ä½¿ç”¨")
    
        # ç»Ÿè®¡ä¿¡æ¯
        end_time = time.time()
        statistics = {
            "source_length": len(src_text),
            "translation_length": len(full_translation),
            "num_chunks": len(chunks),
            "num_terms_extracted": len(terms),
            "num_terms_translated": len(term_dict),
            "terminology_consistent": is_consistent,
            "num_inconsistencies": len(inconsistencies),
            "time_elapsed": round(end_time - start_time, 2),
            "avg_time_per_chunk": round((end_time - start_time) / len(chunks), 2),
            "glossary_provided": glossary is not None,
            "parallel_enabled": parallel
        }
    
        print(f"\n{'='*60}")
        print(f"ç¿»è¯‘å®Œæˆï¼")
        print(f"{'='*60}")
        print(f"ç¿»è¯‘æ¨¡å¼: {'âš¡ å¹¶è¡Œç¿»è¯‘' if parallel else 'ğŸŒ é¡ºåºç¿»è¯‘'}")
        print(f"æ€»è€—æ—¶: {statistics['time_elapsed']} ç§’")
        print(f"å¹³å‡æ¯å—è€—æ—¶: {statistics['avg_time_per_chunk']} ç§’\n")
    
        return {
            "translation": full_translation,
            "term_dict": term_dict,
            "chunks_info": [{"chunk_id": c["chunk_id"], "length": len(c["text"])} for c in chunks],
            "statistics": statistics
        }