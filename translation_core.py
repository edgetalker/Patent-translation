"""
ç¿»è¯‘æ ¸å¿ƒæ¨¡å—
å¤„ç†é•¿æ–‡æ¡£ç¿»è¯‘å’Œæœ¯è¯­ä¸€è‡´æ€§éªŒè¯
é›†æˆè¯­æ–™åº“æ£€ç´¢åŠŸèƒ½
"""
import time
import asyncio
import concurrent.futures
import threading
from typing import Dict, List, Tuple, Optional
from openai import OpenAI

from config import config
from utils import split_text_by_paragraph
from terminology_extraction import TerminologyExtractor

class DocumentTranslator:
    """æ–‡æ¡£ç¿»è¯‘å™¨ï¼ˆæ”¯æŒè¯­æ–™åº“åŠ é€Ÿï¼‰"""
    
    def __init__(self, corpus_manager=None):
        """
        åˆå§‹åŒ–ç¿»è¯‘å™¨
        
        Args:
            corpus_manager: å¯é€‰çš„è¯­æ–™åº“ç®¡ç†å™¨
        """
        self.client = OpenAI(
            api_key=config.LLM_API_KEY,
            base_url=config.LLM_BASE_URL
        )
        self.model_name = config.LLM_MODEL_NAME
        self.term_extractor = TerminologyExtractor()
        self.log_lock = threading.Lock()
        
        # ğŸ†• è¯­æ–™åº“æ”¯æŒ
        self.corpus_manager = corpus_manager
        self.corpus_retriever = None  # å»¶è¿Ÿåˆå§‹åŒ–
    
    def _safe_print(self, *args, **kwargs):
        """çº¿ç¨‹å®‰å…¨çš„æ‰“å°"""
        with self.log_lock:
            print(*args, **kwargs)
            
    def _translate_sentences(
        self,
        sentences: List[Tuple[int, str]],
        src_lang: str,
        tgt_lang: str,
        domain: str,
        term_dict: Dict[str, str] = None,
        domain_prompt: str = None,
        chunk_id: int = 0
    ) -> Dict[int, str]:
        """
        ç¿»è¯‘å¥å­åˆ—è¡¨ï¼ˆæ‰¹é‡ï¼‰
        
        Args:
            sentences: [(index, sentence), ...]
            src_lang: æºè¯­è¨€
            tgt_lang: ç›®æ ‡è¯­è¨€
            domain: é¢†åŸŸ
            term_dict: æœ¯è¯­å­—å…¸
            domain_prompt: é¢†åŸŸæç¤ºè¯
            chunk_id: å—IDï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
        Returns:
            {index: translation}
        """
        if not sentences:
            return {}
        
        src_lang_name = config.get_language_name(src_lang)
        tgt_lang_name = config.get_language_name(tgt_lang)
        
        # æ‹¼æ¥æ‰€æœ‰å¥å­
        sentence_texts = [sent for _, sent in sentences]
        batch_text = "\n".join(sentence_texts)
        
        # æ„å»ºæç¤ºè¯
        prompt_parts = [
            f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{domain}é¢†åŸŸ{src_lang_name}-{tgt_lang_name}ç¿»è¯‘ä¸“å®¶ã€‚"
        ]
        
        if term_dict:
            terms_list = "\n".join([f"  - {src} â†’ {tgt}" for src, tgt in term_dict.items()])
            prompt_parts.append(
                f"\nã€æœ¯è¯­è¡¨ã€‘ä»¥ä¸‹æ˜¯ä¸“ä¸šæœ¯è¯­å¯¹ç…§è¡¨ï¼š\n{terms_list}\n"
                f"\nã€ç¿»è¯‘è¦æ±‚ã€‘"
                f"\n1. ä¸¥æ ¼éµå®ˆæœ¯è¯­è¡¨çš„ç¿»è¯‘è§„èŒƒ"
                f"\n2. ä¿æŒå¥å­çš„ç‹¬ç«‹æ€§ï¼Œæ¯å¥ä¸€è¡Œ"
                f"\n3. ç¦æ­¢ä½¿ç”¨ä»»ä½•æ ¼å¼æ ‡è®°"
                f"\n4. æŒ‰åŸé¡ºåºè¾“å‡ºè¯‘æ–‡"
            )
        
        if domain_prompt:
            prompt_parts.append(domain_prompt)
            
        prompt_parts.append(
            f"\nè¯·å°†ä»¥ä¸‹{len(sentence_texts)}å¥{src_lang_name}ç¿»è¯‘æˆ{tgt_lang_name}ï¼Œ"
            f"æ¯å¥ä¸€è¡Œï¼ŒæŒ‰é¡ºåºè¾“å‡ºï¼š\n\n{batch_text}"
        )
        
        prompt = "".join(prompt_parts)
        
        # è°ƒç”¨LLM
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": f"ä½ æ˜¯ä¸“ä¸šçš„{domain}é¢†åŸŸç¿»è¯‘ä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=config.TRANSLATION_TEMPERATURE,
                max_tokens=config.MAX_TOKENS
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # åˆ†å‰²ç»“æœï¼ˆæŒ‰è¡Œï¼‰
            result_lines = [line.strip() for line in result_text.split('\n') if line.strip()]
            
            # æ˜ å°„å›ç´¢å¼•
            translations = {}
            for i, (idx, _) in enumerate(sentences):
                if i < len(result_lines):
                    translations[idx] = result_lines[i]
                else:
                    # ç¿»è¯‘ç»“æœä¸è¶³ï¼Œä½¿ç”¨åŸæ–‡
                    translations[idx] = sentences[i][1]
            
            return translations
            
        except Exception as e:
            print(f"  âš ï¸  æ‰¹é‡ç¿»è¯‘å¤±è´¥: {str(e)}")
            # è¿”å›åŸæ–‡
            return {idx: sent for idx, sent in sentences}
    
    def translate_chunk(
        self,
        chunk_text: str,
        chunk_id: int,
        total_chunks: int,
        src_lang: str,
        tgt_lang: str,
        domain: str,
        term_dict: Dict[str, str] = None,
        domain_prompt: str = None,
        context: str = None,
        # è¯­æ–™åº“å‚æ•°
        use_corpus: bool = False,
        corpus_id: str = None,
        corpus_threshold: float = 0.85
    ) -> Tuple[str, Optional[Dict]]:
        """
        ç¿»è¯‘å•ä¸ªæ–‡æœ¬å—ï¼ˆæ”¯æŒè¯­æ–™åº“æ£€ç´¢ï¼‰
        
        Args:
            chunk_text: å¾…ç¿»è¯‘æ–‡æœ¬
            chunk_id: å½“å‰å—ID
            total_chunks: æ€»å—æ•°
            src_lang: æºè¯­è¨€ä»£ç 
            tgt_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            domain: é¢†åŸŸä¿¡æ¯
            term_dict: æœ¯è¯­å¯¹ç…§å­—å…¸
            domain_prompt: é¢†åŸŸæç¤ºè¯
            context: å‰æ–‡ä¸Šä¸‹æ–‡
            use_corpus: æ˜¯å¦ä½¿ç”¨è¯­æ–™åº“æ£€ç´¢
            corpus_id: è¯­æ–™åº“ID
            corpus_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            (ç¿»è¯‘ç»“æœ, è¯­æ–™åº“ç»Ÿè®¡ä¿¡æ¯ or None)
        """
        chunk_start = time.time()
        
        # è¯­æ–™åº“æ£€ç´¢åˆ†æ”¯
        if use_corpus and self.corpus_retriever and corpus_id:
            return self._translate_chunk_with_corpus(
                chunk_text=chunk_text,
                chunk_id=chunk_id,
                total_chunks=total_chunks,
                src_lang=src_lang,
                tgt_lang=tgt_lang,
                domain=domain,
                term_dict=term_dict,
                domain_prompt=domain_prompt,
                corpus_id=corpus_id,
                corpus_threshold=corpus_threshold,
                chunk_start=chunk_start
            )
        
        src_lang_name = config.get_language_name(src_lang)
        tgt_lang_name = config.get_language_name(tgt_lang)
        
        prompt_parts = [
            f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{domain}é¢†åŸŸ{src_lang_name}-{tgt_lang_name}ç¿»è¯‘ä¸“å®¶ã€‚",
            f"\nå½“å‰æ­£åœ¨ç¿»è¯‘ç¬¬ {chunk_id + 1} æ®µï¼Œå…± {total_chunks} æ®µã€‚"
        ]
        
        if term_dict:
            relevant_terms = self._get_relevant_terms(
                chunk_text=chunk_text,
                term_dict=term_dict,
                max_inject=config.MAX_INJECT_TERMS
            )

            chunk_lower = chunk_text.lower()
            exact_matches = sum(1 for src in relevant_terms if src.lower() in chunk_lower)
    
            terms_list = "\n".join([f"  - {src} â†’ {tgt}" for src, tgt in relevant_terms.items()])
    
            prompt_parts.append(
                f"\nã€æœ¯è¯­è¡¨ã€‘ä»¥ä¸‹æ˜¯{domain}é¢†åŸŸçš„ä¸“ä¸šæœ¯è¯­å¯¹ç…§è¡¨ï¼ˆå…±{len(term_dict)}ä¸ªï¼‰ï¼š\n{terms_list}\n"
                f"\nã€é‡è¦ç¿»è¯‘è¦æ±‚ã€‘"
                f"\n1. å½“å¾…ç¿»è¯‘æ–‡æœ¬ä¸­å‡ºç°æœ¯è¯­è¡¨ä¸­çš„æœ¯è¯­ï¼ˆåŒ…æ‹¬å…¶å˜å½¢ã€å¤æ•°ã€æ—¶æ€ã€è¯ç»„ç­‰ä»»ä½•å½¢å¼ï¼‰æ—¶ï¼Œå¿…é¡»ä¸¥æ ¼ä½¿ç”¨æŒ‡å®šçš„{tgt_lang_name}ç¿»è¯‘"
                f"\n2. æ³¨æ„è¯†åˆ«æœ¯è¯­çš„å„ç§å˜ä½“å½¢å¼ï¼š"
                f"\n   - è‹±æ–‡ï¼šå•å¤æ•°å˜åŒ–ã€åŠ¨è¯æ—¶æ€ã€æ´¾ç”Ÿè¯ç­‰ï¼ˆå¦‚ optimize/optimizes/optimization éƒ½å¯¹åº”åŒä¸€æœ¯è¯­ï¼‰"
                f"\n   - ä¸­æ–‡ï¼šè¯ç»„åŒ…å«å…³ç³»ï¼ˆå¦‚ 'æœºå™¨å­¦ä¹ ç®—æ³•' åŒ…å« 'æœºå™¨å­¦ä¹ ' æœ¯è¯­ï¼‰"
                f"\n   - å…¶ä»–è¯­è¨€ï¼šæ ¹æ®è¯¥è¯­è¨€çš„è¯­æ³•ç‰¹ç‚¹çµæ´»åŒ¹é…"
                f"\n3. å³ä½¿æœ¯è¯­åœ¨åŸæ–‡ä¸­åªä»¥éƒ¨åˆ†å½¢å¼å‡ºç°ï¼Œä¹Ÿè¦ä¿æŒè¯‘æ–‡çš„æœ¯è¯­ä¸€è‡´æ€§"
                f"\n4. å¯¹äºå¤šè¯æœ¯è¯­ï¼Œç¡®ä¿æ•´ä½“ç¿»è¯‘çš„å‡†ç¡®æ€§"
            )
            
            print(f" Chunk {chunk_id+1}: æœ¯è¯­è¡¨{len(term_dict)}ä¸ª, ç²¾ç¡®åŒ¹é…{exact_matches}ä¸ª â†’ LLMå°†çµæ´»åŒ¹é…å…¨éƒ¨")
        
        if domain_prompt:
            prompt_parts.append(domain_prompt)

        if context:
            prompt_parts.append(f"\nã€å‰æ–‡å‚è€ƒã€‘\n{context[:200]}...\n")
        
        prompt_parts.append(
            f"\nè¯·å°†ä»¥ä¸‹{src_lang_name}å†…å®¹ç¿»è¯‘æˆ{tgt_lang_name}ï¼š"
            "\n1. ä¸¥æ ¼éµå®ˆä¸Šè¿°æœ¯è¯­è¡¨çš„ç¿»è¯‘è§„èŒƒ"
            "\n2. ä¿æŒæ–‡æ¡£çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§"
            "\n3. ä¿æŒåŸæ–‡çš„æ®µè½ç»“æ„"
            "\n4. **ç¦æ­¢ä½¿ç”¨ä»»ä½•æ ¼å¼æ ‡è®°**ï¼šä¸è¦è¾“å‡ºmarkdownæ ¼å¼ï¼ˆå¦‚ ** __ * _ # ç­‰ï¼‰ã€HTMLæ ‡ç­¾æˆ–å…¶ä»–ä»»ä½•æ ¼å¼ç¬¦å·"
            "\n5. è¾“å‡ºçº¯æ–‡æœ¬è¯‘æ–‡ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€è¯´æ˜æˆ–æ ¼å¼ä¿®é¥°"
            f"\n\nã€å¾…ç¿»è¯‘å†…å®¹ã€‘\n{chunk_text}"
        )
        
        prompt = "".join(prompt_parts)
        prompt_time = time.time() - chunk_start
        
        for attempt in range(config.MAX_RETRIES):
            try:
                api_start = time.time()
                
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{domain}é¢†åŸŸç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿{src_lang_name}åˆ°{tgt_lang_name}çš„ç¿»è¯‘ã€‚ä½ å¯¹æœ¯è¯­çš„å„ç§å˜å½¢å½¢å¼æœ‰æ·±åˆ»ç†è§£ï¼Œèƒ½å¤Ÿçµæ´»åŒ¹é…å¹¶ä¿æŒç¿»è¯‘ä¸€è‡´æ€§ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=config.TRANSLATION_TEMPERATURE,
                    max_tokens=config.MAX_TOKENS
                )
                
                api_time = time.time() - api_start
                translation = response.choices[0].message.content.strip()
                total_time = time.time() - chunk_start
                
                print(f"   â±ï¸  Chunk {chunk_id+1} è€—æ—¶: Promptæ„å»º{prompt_time:.2f}s + APIè°ƒç”¨{api_time:.2f}s = {total_time:.2f}s")
                print(f"      è¾“å…¥{len(chunk_text)}å­— â†’ è¾“å‡º{len(translation)}å­— ({len(translation)/len(chunk_text):.2f}x)")
                
                return translation, None  
                
            except Exception as e:
                print(f"  âš ï¸  ç¿»è¯‘chunk {chunk_id + 1} å¤±è´¥ (å°è¯• {attempt + 1}/{config.MAX_RETRIES}): {str(e)}")
                if attempt < config.MAX_RETRIES - 1:
                    time.sleep(config.RETRY_DELAY)
                else:
                    print(f"  âŒ ç¿»è¯‘chunk {chunk_id + 1} æœ€ç»ˆå¤±è´¥ï¼Œè¿”å›åŸæ–‡")
                    return f"[TRANSLATION FAILED: {chunk_text}]", None
    
    def _translate_chunk_with_corpus(
        self,
        chunk_text: str,
        chunk_id: int,
        total_chunks: int,
        src_lang: str,
        tgt_lang: str,
        domain: str,
        term_dict: Dict[str, str],
        domain_prompt: str,
        corpus_id: str,
        corpus_threshold: float,
        chunk_start: float
    ) -> Tuple[str, Dict]:
        """
        ä½¿ç”¨è¯­æ–™åº“æ£€ç´¢çš„chunkç¿»è¯‘
        
        Returns:
            (ç¿»è¯‘ç»“æœ, ç»Ÿè®¡ä¿¡æ¯)
        """
        self._safe_print(f"\n   ğŸ” Chunk {chunk_id+1}: æ£€ç´¢è¯­æ–™åº“...")
        
        try:
            retrieval_result = asyncio.run(
                self.corpus_retriever.retrieve_for_chunk(
                    chunk=chunk_text,
                    corpus_id=corpus_id,
                    threshold=corpus_threshold
                )
            )    
        except RuntimeError as e:
            if "cannot be called when another event loop is running" in str(e):
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as pool:
                    future = pool.submit(
                        asyncio.run,
                        self.corpus_retriever.retrieve_for_chunk(
                            chunk=chunk_text,
                            corpus_id=corpus_id, 
                            threshold=corpus_threshold
                        )
                    )
                    retrieval_result = future.result()
            else:
                raise
        except Exception as e:
            self._safe_print(f"   âš ï¸  æ£€ç´¢å¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°ç›´æ¥ç¿»è¯‘")
            translation, _ = self.translate_chunk(
                chunk_text=chunk_text,
                chunk_id=chunk_id,
                total_chunks=total_chunks,
                src_lang=src_lang,
                tgt_lang=tgt_lang,
                domain=domain,
                domain_prompt=domain_prompt,
                term_dict=term_dict,
                use_corpus=False
            )
            return translation, None
        
        # 2. ç»Ÿè®¡
        self._safe_print(f"      âœ“ åˆ†å¥: {len(retrieval_result.sentences)}å¥")
        self._safe_print(f"      âœ“ å‘½ä¸­: {retrieval_result.hit_count}å¥ ({retrieval_result.hit_rate*100:.1f}%)")
        self._safe_print(f"      âœ“ æœªå‘½ä¸­: {retrieval_result.miss_count}å¥")
        
        # 3. è·å–æœªå‘½ä¸­çš„å¥å­
        unmatched = retrieval_result.get_unmatched_sentences()
        
        # 4. ç¿»è¯‘æœªå‘½ä¸­çš„å¥å­
        llm_translations = {}
        if unmatched:
            self._safe_print(f"      ğŸ¤– LLMç¿»è¯‘ {len(unmatched)} å¥...")
            llm_start = time.time()

            unmatched_text = "\n".join(sent for _, sent in unmatched)

            relevant_terms = self._get_relevant_terms(
                chunk_text=unmatched_text,
                term_dict=term_dict,
                max_inject=config.MAX_INJECT_TERMS
            ) if term_dict else None

            llm_translations = self._translate_sentences(
                sentences=unmatched,
                src_lang=src_lang,
                tgt_lang=tgt_lang,
                domain=domain,
                term_dict=relevant_terms,
                domain_prompt=domain_prompt,
                chunk_id=chunk_id
            )
            
            llm_time = time.time() - llm_start
            self._safe_print(f"      âœ“ LLMç¿»è¯‘è€—æ—¶: {llm_time:.2f}s")
        else:
            self._safe_print(f"      âš¡ å…¨éƒ¨å‘½ä¸­ï¼Œè·³è¿‡LLMç¿»è¯‘")
        
        # 5. åˆå¹¶ç»“æœ
        translation = self.corpus_retriever.merge_translation(
            retrieval_result=retrieval_result,
            llm_translations=llm_translations
        )
        
        total_time = time.time() - chunk_start
        self._safe_print(f"   â±ï¸  Chunk {chunk_id+1} æ€»è€—æ—¶: {total_time:.2f}s")
        self._safe_print(f"      è¾“å…¥{len(chunk_text)}å­— â†’ è¾“å‡º{len(translation)}å­—")
        
        # 6. è¿”å›ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_sentences": len(retrieval_result.sentences),
            "hits": retrieval_result.hit_count,
            "misses": retrieval_result.miss_count,
            "hit_rate": retrieval_result.hit_rate,
            "time": total_time
        }
        
        return translation, stats
    
    def _get_relevant_terms(
        self,
        chunk_text: str,
        term_dict: Dict[str, str],
        max_inject: int = 25
    ) -> Dict[str, str]:
        """
        è¯­è¨€æ— å…³çš„æœ¯è¯­è¿‡æ»¤ï¼šç²¾ç¡®åŒ¹é…ä¼˜å…ˆ + é¢‘ç‡ä¿åº•
        
        Args:
            chunk_text: å½“å‰ chunk æ–‡æœ¬
            term_dict: å®Œæ•´æœ¯è¯­å­—å…¸ï¼ˆå·²æŒ‰æå–é¢‘ç‡æ’åºï¼‰
            max_inject: æœ€å¤§æ³¨å…¥æ•°é‡
        
        Returns:
            è¿‡æ»¤åçš„æœ¯è¯­å­—å…¸
        """
        chunk_lower = chunk_text.lower()
        
        # ç¬¬ä¸€å±‚ï¼šç²¾ç¡®å­ä¸²åŒ¹é…ï¼Œè¯­è¨€æ— å…³
        matched = {
            src: tgt for src, tgt in term_dict.items()
            if src.lower() in chunk_lower
        }
        
        # ç¬¬äºŒå±‚ï¼šç²¾ç¡®åŒ¹é…ä¸è¶³ä¸Šé™æ—¶ï¼ŒæŒ‰é¢‘ç‡é¡ºåºè¡¥è¶³
        if len(matched) < max_inject:
            for src, tgt in term_dict.items():
                if src not in matched:
                    matched[src] = tgt
                if len(matched) >= max_inject:
                    break
        
        return matched
    
    def validate_terminology_consistency(
        self,
        translation: str,
        term_dict: Dict[str, str],
        src_text: str,
        tgt_lang: str
    ) -> Tuple[bool, List[str]]:
        """éªŒè¯ç¿»è¯‘ç»“æœä¸­çš„æœ¯è¯­ä¸€è‡´æ€§"""
        inconsistencies = []
        
        for src_term, tgt_term in term_dict.items():
            if src_term not in src_text:
                continue
            
            tgt_variants = [
                tgt_term,
                tgt_term.lower(),
                tgt_term.capitalize(),
            ]
            
            if tgt_lang == 'en':
                if tgt_term.endswith('y'):
                    tgt_variants.append(tgt_term[:-1] + 'ies')
                else:
                    tgt_variants.append(tgt_term + 's')
            
            found = any(variant in translation for variant in tgt_variants)
            
            if not found:
                words = tgt_term.lower().split()
                if len(words) > 1:
                    all_words_present = all(word in translation.lower() for word in words)
                    if not all_words_present:
                        inconsistencies.append(f"{src_term} -> {tgt_term}")
                else:
                    inconsistencies.append(f"{src_term} -> {tgt_term}")
        
        is_consistent = len(inconsistencies) == 0
        return is_consistent, inconsistencies
    
    def translate_document(
        self,
        src_text: str,
        src_lang: str,
        tgt_lang: str,
        domain: str = "æŠ€æœ¯",
        use_context: bool = True,
        glossary: Dict[str, str] = None,
        domain_prompt: str = None,
        parallel: bool = True,
        max_workers: int = 3,
        # ğŸ†• è¯­æ–™åº“å‚æ•°
        corpus_id: Optional[str] = None,
        use_corpus: bool = False,
        corpus_threshold: float = 0.85
    ) -> Dict:
        """
        ç¿»è¯‘é•¿æ–‡æ¡£ï¼ˆå¸¦æœ¯è¯­ä¸€è‡´æ€§å¤„ç† + è¯­æ–™åº“åŠ é€Ÿï¼‰
    
        Args:
            src_text: æºæ–‡æœ¬
            src_lang: æºè¯­è¨€ä»£ç 
            tgt_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            domain: é¢†åŸŸä¿¡æ¯
            use_context: æ˜¯å¦ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆå¹¶è¡Œæ¨¡å¼ä¸‹è‡ªåŠ¨ç¦ç”¨ï¼‰
            glossary: æœ¯è¯­å¯¹ç…§å­—å…¸
            domain_prompt: é¢†åŸŸæç¤ºè¯
            parallel: æ˜¯å¦å¯ç”¨å¹¶è¡Œç¿»è¯‘
            max_workers: å¹¶è¡Œç¿»è¯‘çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            corpus_id: è¯­æ–™åº“IDï¼Œä¸ä¼ åˆ™è‡ªåŠ¨ç”Ÿæˆ
            use_corpus: æ˜¯å¦ä½¿ç”¨è¯­æ–™åº“æ£€ç´¢åŠ é€Ÿ
            corpus_threshold: è¯­æ–™åº“ç›¸ä¼¼åº¦é˜ˆå€¼
        
        Returns:
            result: åŒ…å«translation, term_dict, chunks_info, statistics, corpus_stats
        """
        src_lang_name = config.get_language_name(src_lang)
        tgt_lang_name = config.get_language_name(tgt_lang)
    
        print(f"\n{'='*60}")
        print(f"å¼€å§‹ç¿»è¯‘é•¿æ–‡æ¡£ï¼ˆ{src_lang_name} â†’ {tgt_lang_name}ï¼ŒåŸŸï¼š{domain}ï¼‰")
        if use_corpus and self.corpus_manager:
            print(f"âš¡ è¯­æ–™åº“åŠ é€Ÿå·²å¯ç”¨ï¼ˆé˜ˆå€¼: {corpus_threshold}ï¼‰")
        print(f"{'='*60}\n")
    
        start_time = time.time()
        
        # åˆå§‹åŒ–è¯­æ–™åº“æ£€ç´¢å™¨
        if use_corpus and self.corpus_manager:
            from corpus_retrieval import CorpusRetriever
            self.corpus_retriever = CorpusRetriever(
                corpus_manager=self.corpus_manager,
                src_lang=src_lang,
                tgt_lang=tgt_lang
            )
            if not corpus_id:
                corpus_id = f"{src_lang}_{tgt_lang}_{domain}"
        else:
            corpus_id = None
    
        # Step 1 & 2: æœ¯è¯­å¤„ç†
        if glossary:
            print("ğŸ“Œ æ­¥éª¤1-2: ä½¿ç”¨ä¼ å…¥çš„æœ¯è¯­è¡¨...")
            term_dict = glossary
            terms = list(glossary.keys())
            print(f"\n   âœ… ä½¿ç”¨ {len(term_dict)} ä¸ªé¢„å®šä¹‰æœ¯è¯­")
        else:
            print("ğŸ“Œ æ­¥éª¤1: ä½¿ç”¨æ»‘åŠ¨çª—å£æŠ½å–å…³é”®æœ¯è¯­...")
            terms = self.term_extractor.sliding_window_extract(src_text, src_lang, domain)
            print(f"\n   âœ… æœ€ç»ˆæå– {len(terms)} ä¸ªå…³é”®æœ¯è¯­")
            
            print(f"\nğŸ“Œ æ­¥éª¤2: å°†æœ¯è¯­ç¿»è¯‘æˆ{tgt_lang_name}...")
            term_dict = self.term_extractor.translate_terminology(terms, src_lang, tgt_lang, domain)
            print(f"   æˆåŠŸç¿»è¯‘ {len(term_dict)} ä¸ªæœ¯è¯­")
    
        # Step 3: æ–‡æ¡£åˆ†å—
        print("\nğŸ“Œ æ­¥éª¤3: æ–‡æ¡£åˆ†å—...")
        chunks = split_text_by_paragraph(src_text, config.MAX_CHUNK_LENGTH)
        print(f"   æ–‡æ¡£å·²åˆ†ä¸º {len(chunks)} ä¸ªå—")
        
        # è¯­æ–™åº“ç»Ÿè®¡
        corpus_stats = {
            "enabled": use_corpus and self.corpus_manager is not None,
            "total_sentences": 0,
            "total_hits": 0,
            "total_misses": 0,
            "overall_hit_rate": 0.0
        }
    
        # Step 4: ç¿»è¯‘
        if parallel and len(chunks) > 2:
            mode_desc = "âš¡ å¹¶è¡Œç¿»è¯‘"
            if use_corpus and corpus_id:
                mode_desc += " + ğŸ” è¯­æ–™åº“æ£€ç´¢"
            
            print(f"\nğŸ“Œ æ­¥éª¤4: {mode_desc}ï¼ˆ{max_workers}çº¿ç¨‹å¹¶å‘ï¼‰...")
            print(f"   âš¡ é¢„è®¡è€—æ—¶çº¦ä¸ºé¡ºåºç¿»è¯‘çš„ {1/min(max_workers, len(chunks)):.0%}\n")
            
            translations = [None] * len(chunks)
            chunk_stats = [None] * len(chunks)
            
            def translate_task(idx):
                chunk = chunks[idx]
                translation, stats = self.translate_chunk(
                    chunk_text=chunk["text"],
                    chunk_id=idx,
                    total_chunks=len(chunks),
                    src_lang=src_lang,
                    tgt_lang=tgt_lang,
                    domain=domain,
                    term_dict=term_dict,
                    domain_prompt=domain_prompt,
                    context=None,
                    use_corpus=use_corpus,
                    corpus_id=corpus_id,
                    corpus_threshold=corpus_threshold
                )
                return translation, stats
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_idx = {
                    executor.submit(translate_task, i): i
                    for i in range(len(chunks))
                }
                
                completed = 0
                for future in concurrent.futures.as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    try:
                        translation, stats = future.result()
                        translations[idx] = translation
                        chunk_stats[idx] = stats
                        
                        # ç´¯è®¡è¯­æ–™åº“ç»Ÿè®¡
                        if stats:
                            corpus_stats["total_sentences"] += stats["total_sentences"]
                            corpus_stats["total_hits"] += stats["hits"]
                            corpus_stats["total_misses"] += stats["misses"]
                        
                        completed += 1
                        print(f"\n   âœ“ å®Œæˆ {completed}/{len(chunks)} ä¸ªchunks")
                    except Exception as e:
                        print(f"   âŒ Chunk {idx+1} å¤±è´¥: {e}")
                        translations[idx] = f"[TRANSLATION FAILED]"
                        completed += 1
        else:
            mode_desc = "ğŸŒ é¡ºåºç¿»è¯‘"
            if use_corpus and corpus_id:
                mode_desc += " + ğŸ” è¯­æ–™åº“æ£€ç´¢"
            
            print(f"\nğŸ“Œ æ­¥éª¤4: {mode_desc}...")
            translations = []
            context = None
            
            for i, chunk in enumerate(chunks):
                print(f"\n   ç¿»è¯‘ Chunk {i+1}/{len(chunks)}...")
                
                if use_context and i > 0 and not use_corpus:
                    prev_translation = translations[-1]
                    context = prev_translation[-config.OVERLAP_LENGTH:] if len(prev_translation) > config.OVERLAP_LENGTH else prev_translation
                else:
                    context = None
                
                translation, stats = self.translate_chunk(
                    chunk_text=chunk["text"],
                    chunk_id=i,
                    total_chunks=len(chunks),
                    src_lang=src_lang,
                    tgt_lang=tgt_lang,
                    domain=domain,
                    term_dict=term_dict,
                    domain_prompt=domain_prompt,
                    context=context,
                    use_corpus=use_corpus,
                    corpus_id=corpus_id,
                    corpus_threshold=corpus_threshold
                )
                
                translations.append(translation)
                
                # ç´¯è®¡è¯­æ–™åº“ç»Ÿè®¡
                if stats:
                    corpus_stats["total_sentences"] += stats["total_sentences"]
                    corpus_stats["total_hits"] += stats["hits"]
                    corpus_stats["total_misses"] += stats["misses"]
                
                if not use_corpus:
                    print(f"   âœ“ å®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(translation)} å­—ç¬¦")
    
        # è®¡ç®—æ€»å‘½ä¸­ç‡
        if corpus_stats["total_sentences"] > 0:
            corpus_stats["overall_hit_rate"] = corpus_stats["total_hits"] / corpus_stats["total_sentences"]
    
        # Step 5 & 6: ç»„è£…å’ŒéªŒè¯
        print("\nğŸ“Œ æ­¥éª¤5: ç»„è£…ç¿»è¯‘ç»“æœ...")
        full_translation = "\n\n".join(translations)
    
        print("\nğŸ“Œ æ­¥éª¤6: éªŒè¯æœ¯è¯­ä¸€è‡´æ€§...")
        is_consistent, inconsistencies = self.validate_terminology_consistency(
            full_translation, term_dict, src_text, tgt_lang
        )
    
        if is_consistent:
            print("   âœ… æ‰€æœ‰æœ¯è¯­ç¿»è¯‘ä¸€è‡´")
        else:
            print(f"   âš ï¸  å‘ç° {len(inconsistencies)} ä¸ªæœ¯è¯­å¯èƒ½æœªæ­£ç¡®ä½¿ç”¨")
    
        # ç»Ÿè®¡ä¿¡æ¯
        end_time = time.time()
        statistics = {
            "source_length": len(src_text),
            "translation_length": len(full_translation),
            "num_chunks": len(chunks),
            "num_terms_extracted": len(terms),
            "num_terms_translated": len(term_dict),
            "terminology_consistent": is_consistent,
            "num_inconsistencies": len(inconsistencies),
            "time_elapsed": round(end_time - start_time, 2),
            "avg_time_per_chunk": round((end_time - start_time) / len(chunks), 2),
            "glossary_provided": glossary is not None,
            "parallel_enabled": parallel
        }
    
        print(f"\n{'='*60}")
        print(f"ç¿»è¯‘å®Œæˆï¼")
        print(f"{'='*60}")
        print(f"ç¿»è¯‘æ¨¡å¼: {'âš¡ å¹¶è¡Œç¿»è¯‘' if parallel else 'ğŸŒ é¡ºåºç¿»è¯‘'}")
        
        # è¯­æ–™åº“ç»Ÿè®¡è¾“å‡º
        if corpus_stats["enabled"]:
            print(f"è¯­æ–™åº“åŠ é€Ÿ: ğŸ” å·²å¯ç”¨")
            print(f"  - æ€»å¥å­æ•°: {corpus_stats['total_sentences']}")
            print(f"  - å‘½ä¸­æ•°: {corpus_stats['total_hits']}")
            print(f"  - LLMç¿»è¯‘æ•°: {corpus_stats['total_misses']}")
            print(f"  - å‘½ä¸­ç‡: {corpus_stats['overall_hit_rate']*100:.1f}%")
            if corpus_stats['total_sentences'] > 0:
                time_saved = statistics['time_elapsed'] * corpus_stats['overall_hit_rate']
                print(f"  - é¢„è®¡èŠ‚çœæ—¶é—´: ~{time_saved:.1f}ç§’")
        
        print(f"æ€»è€—æ—¶: {statistics['time_elapsed']} ç§’")
        print(f"å¹³å‡æ¯å—è€—æ—¶: {statistics['avg_time_per_chunk']} ç§’\n")
    
        return {
            "translation": full_translation,
            "term_dict": term_dict,
            "chunks_info": [{"chunk_id": c["chunk_id"], "length": len(c["text"])} for c in chunks],
            "statistics": statistics,
            "corpus_stats": corpus_stats
        }